---
title: "Friends Proposal"
author: "Wenyang Cao, Wenling Zhou, Sixuan Li, Haoran Yang"
date: "2024-03-01"
output: 
  html_document: 
    theme: united
    toc: yes
    toc_float: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55), tidy = TRUE)
```

![ ](/Users/yanghaoran/Desktop/Hollywood-Insider-Friends-TV-Sitcom.jpg)

Our project's GitHub repository: <https://github.com/educated-fool/friends>

* Raw Data scraped from here: <https://fangj.github.io/friends/>
*   `friends.R`: R code that can be used to obtain the cleaned dataset from the raw dataset.
*  `friends_quotes.csv`: Cleaned dataset
*   `friends_proposal.html`: Written progress report


## **Introduction**

The popular TV sitcom Friends ran for 10 seasons from 1994 to 2004 and featured the lives of six young people - Rachel, Ross, Monica, Chandler, Phoebe and Joey.  Through their daily interactions, trials and tribulations of living in New York City, the show portrayed complex themes around friendship, love, relationships, career challenges and personal growth. The six friends come from diverse family backgrounds which shape their identities and worldviews.   Audiences connect deeply with the honest portrayal of their flaws and growth.

Past research has analyzed aspects of the characters, story arcs, and audience reception of Friends. However, there remains a gap in understanding the emotional expression and cultural backgrounds reflected in the characters' dialogues. Friends provides a very analytical dataset, which is an analytical dataset to use for text analysis and sentiment analysis. Each character has a unique communication style, word usage, and emotional expression, providing a rich data source for us. Dialogue analysis enables an in-depth understanding of each character's personality traits and social background.


## **Research Questions**

1. Which character in Friends expresses love most frequently or exhibits the most positive attitude based on an analysis of their dialogue?

Current research lacks in providing a detailed analysis of verbal and emotional expression in Friends, particularly in quantifying the distribution of love and positive emotions among the characters. Specifically, we will explore the complexity of the conversation between the characters and relate it to their backgrounds and caring personalities to determine which character shows the most emotional expression and positive attitude. Research in this way can help us analyze the individuality of each character and lead to a deeper understanding of their role and development in the plot of the story.

2. How can text analysis and natural language processing (NLP) methods be utilized to measure and explore the impact of characters' social backgrounds on their speaking styles and vocabulary choices?

Six young people from different families and backgrounds live together in New York City. We will use data science methods to analyze each character's conversations to reveal the different social backgrounds they are living in. For instance, Ross, who possesses a Ph.D. in paleontology, has a significant depth of knowledge in his field. This contrasts with Joey, who lacks a college education. Thus, we want to examine how their social backgrounds differ from a data science perspective.

## **Literature Review**

A number of prior works have analyzed aspects of the Friends TV series related to this study.  Puri (2021) conducted a sentiment analysis of key story arcs and audience reactions posted on Reddit during a 2020 streaming release of the show.  By tallying emotional phrases like cheers, tears, and gasps, they identified the scenes that take fans on a nostalgia rollercoaster ride even on repeated views - like Ross and Rachel finally getting together.

Bizri (2018) compared personality traits between characters using the Big Five personality model over the first 5 seasons.  Differences emerged showing Rachel as more extroverted than someone like Chandler.  However, this study did not connect personality directly to analysis of dialogue patterns and emotional expression itself.

Seth (2017) analyzed scripts to determine how prominent each character was based on their total word count and number of lines.  While counting words gives a measure of "talkativeness", it does not provide insight into the actual content and sentiment of speech.  Our linguistic analysis will build on these basics to assess emotionality and cultural influences.

By analyzing Friends character dialogues for both emotional expression and cultural influences, we can develop a deeper understanding of what each main cast member brought to one of TV’s most iconic sitcoms.  This methodology could be extended to other popular sitcoms to compare similarities and differences.

## **Code**

### Part I: parse_and_scrape function

```{r eval=FALSE}
##### ##### ##### ##### ##### ##### 
##### Scraping the Data ##### ##### 
##### ##### ##### ##### ##### ##### 

##### ##### ##### ##### ##### ##### 
#####       Part I      ##### ##### 
##### ##### ##### ##### ##### ##### 

##### ##### ##### ##### ##### ##### 
# Part I: parse_and_scrape function
##### ##### ##### ##### ##### ##### 

# Load necessary libraries
library(rvest)
library(dplyr)
library(stringr)
library(purrr)

# Base URL for the main Friends transcript page
base_url <- "https://fangj.github.io/friends/"

# Function to parse episode details and extract dialogues
parse_and_scrape <- function(html_line) {
  # Extract the href attribute and text
  link <- html_attr(html_line, "href")
  text <- html_text(html_line)
  
  # Construct full link
  full_link <- paste0(base_url, link)
  
  # Adjusted to handle season 10 and special cases like 212-213 for episode numbers
  # Now also handles ranges like 1017-1018
  if(str_detect(link, "-")) {
    match_data <- str_match(link, "season/(\\d{2})(\\d{2})-(\\d{2})\\.html")
    season <- as.integer(match_data[, 2])
    episode <- as.integer(match_data[, 3]) # Only the first episode in the range
    episode_number <- sprintf("S%02dE%02d", season, episode)
  } else {
    match_data <- str_match(link, "season/(\\d{2})(\\d{2})\\.html")
    season <- as.integer(match_data[, 2])
    episode <- as.integer(match_data[, 3])
    episode_number <- sprintf("S%02dE%02d", season, episode)
  }
  
  title <- str_extract(text, "(?<=\\d\\s|-\\d\\s).*$") # Updated regex to handle ranges
  
  # Scrape dialogues from the episode's page
  page <- read_html(full_link)
  dialogues <- page %>%
    html_nodes("p") %>%
    html_text() %>%
    .[str_detect(., regex("^(Monica|Joey|Chandler|Phoebe|Ross|Rachel):", ignore_case = TRUE))]
  
  if (length(dialogues) == 0) {
    return(tibble())
  }
  
  authors <- str_extract(dialogues, "^[A-Za-z]+")
  quotes <- str_replace_all(dialogues, "^[A-Za-z]+:", "") %>%
    str_trim()
  quote_order <- seq_along(quotes)
  
  data_frame <- tibble(
    season = season,
    episode = episode,
    episode_number = episode_number,
    title = title,
    author = authors,
    quote = quotes,
    quote_order = quote_order
  )
  
  return(data_frame)
}

# Read the main page HTML and extract episode links
main_page <- read_html(base_url)
episode_links <- html_nodes(main_page, "ul li a")

# Parse episode details and scrape dialogues for each episode link
dialogues_data <- map_df(episode_links, parse_and_scrape)
```

### Part II: parse_single_episode function

```{r eval=FALSE}
##### ##### ##### ##### ##### ##### 
#####       Part II     ##### ##### 
##### ##### ##### ##### ##### ##### 

##### ##### ##### ##### ##### ##### 
# Part II: parse_single_episode function
##### ##### ##### ##### ##### ##### 

#Due to missing lines in the HTML source of episodes 3-24 of season 2, caused by <br> tags not 
#being captured during the initial scrape, these episodes were excluded from the original dataset. 
#A new function has been utilized to accurately scrape and incorporate the data from these pages.

dialogues_data_wo_s2 <- dialogues_data %>%
  filter(!(!is.na(season) & season == 2 & episode >= 3 & episode <= 24))

# Function to parse a single episode's page
parse_single_episode <- function(episode_url) {
  # Read the HTML content of the page
  page_content <- read_html(episode_url)
  
  # Extract the title of the episode
  title <- page_content %>%
    html_nodes("title") %>%
    html_text() %>%
    str_trim()
  
  # Extract all text from the page
  text_all <- page_content %>%
    html_nodes("body") %>%
    html_text()
  
  # Use regular expression to find dialogues and split text into lines
  dialogues_lines <- unlist(str_split(text_all, "\r\n|\n|\r"))
  
  # Filter lines that represent dialogues
  dialogue_lines <- dialogues_lines[str_detect(dialogues_lines, "^(JOEY|CHANDLER|MONICA|PHOEBE|ROSS|RACHEL):")]
  
  # Extract author and quote
  authors <- str_extract(dialogue_lines, "^[A-Z]+") %>% 
    tolower() %>% 
    str_to_title()
  
  quotes <- str_replace(dialogue_lines, "^[A-Z]+:", "")
  
  # Generate quote order
  quote_order <- seq_along(quotes)
  
  # Handle special case for "0212-0213"
  if (grepl("0212-0213.html", episode_url)) {
    season <- NA
    episode <- NA
    episode_number <- NA
  } else {
    # Extract season and episode from URL
    url_parts <- str_extract(episode_url, "(\\d{2})(\\d{2})\\.html$")
    season <- as.integer(substr(url_parts, 1, 2))
    episode <- as.integer(substr(url_parts, 3, 4))
    episode_number <- sprintf("S%02dE%02d", season, episode)
  }
  
  # Create a dataframe
  data_frame <- tibble(
    season = rep(season, length(quote_order)),
    episode = rep(episode, length(quote_order)),
    episode_number = rep(episode_number, length(quote_order)),
    title = rep(title, length(quote_order)),
    author = authors,
    quote = quotes,
    quote_order = quote_order
  )
  
  return(data_frame)
}

# List of episode URLs
episode_urls <- c(
  "https://fangj.github.io/friends/season/0203.html",
  "https://fangj.github.io/friends/season/0204.html",
  "https://fangj.github.io/friends/season/0205.html",
  "https://fangj.github.io/friends/season/0206.html",
  "https://fangj.github.io/friends/season/0207.html",
  "https://fangj.github.io/friends/season/0208.html",
  "https://fangj.github.io/friends/season/0209.html",
  "https://fangj.github.io/friends/season/0210.html",
  "https://fangj.github.io/friends/season/0211.html",
  "https://fangj.github.io/friends/season/0212-0213.html",
  "https://fangj.github.io/friends/season/0214.html",
  "https://fangj.github.io/friends/season/0215.html",
  "https://fangj.github.io/friends/season/0216.html",
  "https://fangj.github.io/friends/season/0217.html",
  "https://fangj.github.io/friends/season/0218.html",
  "https://fangj.github.io/friends/season/0219.html",
  "https://fangj.github.io/friends/season/0220.html",
  "https://fangj.github.io/friends/season/0221.html",
  "https://fangj.github.io/friends/season/0222.html",
  "https://fangj.github.io/friends/season/0223.html",
  "https://fangj.github.io/friends/season/0224.html"
)

# Process each episode and combine data
dialogues_data_s2 <- map_df(episode_urls, parse_single_episode)

```

### Part III: Merge and Clean

```{r eval=FALSE}



##### ##### ##### ##### ##### ##### 
#####       Part III    ##### ##### 
##### ##### ##### ##### ##### ##### 


##### ##### ##### ##### ##### ##### 
# Part III: Merge and Clean
##### ##### ##### ##### ##### ##### 

# Merge dialogues_data_wo_s2 and dialogues_data_s2
df <- bind_rows(dialogues_data_wo_s2, dialogues_data_s2)

# Convert author names to Title Case
df <- df %>%
  mutate(author = str_to_title(tolower(author)))

# Convert special cases
df <- df %>%
  mutate(
    season = case_when(
      title == "In Barbados" ~ 9,
      title == "That Could Have Been, Part I & II" ~ 6,
      title == "The Last One, Part I & II" ~ 10,
      title == "outtakesFriends Special: The Stuff You've Never Seen" ~ 7,
      title == "The One After the Superbowl" ~ 2,
      TRUE ~ season
    ),
    episode = case_when(
      title == "In Barbados" ~ 23,
      title == "That Could Have Been, Part I & II" ~ 15,
      title == "The Last One, Part I & II" ~ 17,
      title == "outtakesFriends Special: The Stuff You've Never Seen" ~ 24,
      title == "The One After the Superbowl" ~ 12,
      TRUE ~ episode
    ),
    episode_number = case_when(
      title == "In Barbados" ~ "S09E23",
      title == "That Could Have Been, Part I & II" ~ "S06E15",
      title == "The Last One, Part I & II" ~ "S10E17",
      title == "outtakesFriends Special: The Stuff You've Never Seen" ~ "S07E24",
      title == "The One After the Superbowl" ~ "S02E12",
      TRUE ~ episode_number
    )
  )

# Calculate the number of quotes per episode
quotes_count_per_episode <- df %>%
  group_by(season, episode, episode_number, title) %>%
  summarise(quotes_count = n())

# Display the result
print(quotes_count_per_episode)

# Display the unique authors
print(unique(df$author))

```

### Part IV: CSV and ZIP Files

```{r eval=FALSE}



##### ##### ##### ##### ##### ##### 
#####       Part IV     ##### ##### 
##### ##### ##### ##### ##### ##### 

##### ##### ##### ##### ##### ##### 
# Part IV: CSV and ZIP Files
##### ##### ##### ##### ##### ##### 

# Write the dataframe to a CSV file
write.csv(df, "friends_quotes.csv", row.names = FALSE)

# Compress the CSV file into a ZIP file
zip(zipfile = "friends_quotes.zip", files = "friends_quotes.csv")
```

## **Data**
### Raw Data Description:
Our team plans to use R to scrape quotes from the script of Friends, setting the process to obtain the exact variables needed for the analysis. This ensures the accuracy and flexibility of the data to fit our research goals.

We aim to extract the 7 specific variables from the raw data：

* `Season`: The number representing the season of the quote
* `Episode`: The name of the episode where the quote is from
* `Episode_number`: The episode number within the season
* `Title`: The episode title
* `Author`: The character who said the quote
* `Quote`: The dialogue spoken by a character
* `Quote_order`: The order of the quote within the episode

By scraping these seven variables, we can create a comprehensive dataset containing each quote's necessary contextual information (season, episode, character). This dataset will enable us to conduct a thorough analysis of dialogue based on season, episode, and character, aligning with our text analysis research objectives.

### Efforts to prepare the data

#### 1. Part I: parse_and_scrape function

We attempted to scrape `episode_number` from the raw data, and the results showed that while most episode titles correspond to one episode, some plot threads may span two episodes with the same title. For this situation, we processed the episode links to ensure that only the first episode in each episode range is displayed.

For example, for episodes that combine two episodes into one, such as "The One After the Superbowl," the URL format is [https://fangj.github.io/friends/season/0212-0213.html](https://fangj.github.io/friends/season/0212-0213.html).
A link containing "-" represents a range of episodes. In this case, we used an `if-else` conditional statement to parse the first episode number in the range as the `episode_number`. In the code, we employed a formal expression to extract the season number and the first episode number in the range from the link and format it as the correct episode number. Thus, for the URL [https://fangj.github.io/friends/season/0212-0213.html](https://fangj.github.io/friends/season/0212-0213.html), we parsed the episode number as `S02E12`.

In addition, we addressed the case sensitivity of author names during the dialogue extraction process. The `str_detect` function in the `stringr` package is used to search within each vector element. The pattern `^(Monica|Joey|Chandler|Phoebe|Ross|Rachel)` matches any line that begins with these six names. However, our team noticed that some lines of dialogue where the author's name begins with a capital letter needed to be captured. To address this issue, we used the `ignore_case = TRUE` parameter in the `str_detect` function, which makes the search process case-insensitive. We ensured that regardless of whether authors' names were in uppercase or lowercase, they would be correctly matched to their respective dialogues and included in our dataset.


#### 2. Part II: parse_single_episode function

Some lines are missing from the HTML source for episodes 3 through 24 of Season 2 due to the `<br>` tags needing to be captured correctly. Some of the data from these episodes was not captured during the initial scraping process, so they were excluded from the original dataset. To address this issue, we created a new function that could accurately recapture the missing data from these episodes and merge it into the original dataset. This new function allows us to collect complete dialog data for all episodes of Season 2, ensuring the completeness and accuracy of the data and providing a more reliable foundation for further analysis and research.


#### 3. Part III: Merge and Clean

First, we used the `bind_rows()` function to merge two datasets, `dialogues_data_wo_s2` and `dialogues_data_s2`, into a single dataframe named "df" to merge the dialogues data from all the episodes of the second season.

After the data was merged, we reviewed the formatting of the authors' names and found some inconsistencies. To ensure the consistency of the data, we used the `mutate()` function and the `str_to_title()` function to convert all author names to Title Case format, a form of initial capitalization. By doing so, we can ensure a uniform format of author names and make it easier to process and analyze the data further.

For some cases where two episodes were merged into one, we could not successfully capture the `episode_number`. To deal with this, we used the `case_when` function to determine the `episode_number` based on the episode's title. We manually specify the `episode_number` for a specific episode title to ensure the integrity and accuracy of the data. If the episode title does not match a particular case, the original `episode_number` value is retained.

Finally, we output two tables to summarize the final extraction: `quotes_count_per_episode` shows the number of conversations in each episode, and `unique(df$author)` lists all the individual authors in the dataset.

## **Work Cited**

- Jenny B. '25. “I’m Not Normal about Friends (Part 1).” MIT Admissions, 25 Nov. 2023, [mitadmissions.org/blogs/entry/im-not-normal-about-friends-part-1](https://mitadmissions.org/blogs/entry/im-not-normal-about-friends-part-1/#:~:text=Ross%20and%20Rachel%20both%20said). Accessed 25 Feb. 2024.
- Sahakyan, Elizabeth Ter. “The One with the Data Scientist: A Closer Look at the Friends of Friends.” Medium, 28 June 2019, [medium.com/@liztersahakyan/the-one-with-the-data-scientist-a-closer-look-at-the-friends-of-friends-d3530d1902af](https://medium.com/@liztersahakyan/the-one-with-the-data-scientist-a-closer-look-at-the-friends-of-friends-d3530d1902af). Accessed 25 Feb. 2024.


